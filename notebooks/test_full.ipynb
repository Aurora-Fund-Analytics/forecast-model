{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aurora-Fund-Analytics/forecast-model/blob/main/notebooks/test_full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "# 1. Install dependencies\n",
        "# =====================================\n",
        "!pip install yfinance ta torch scikit-learn matplotlib --quiet\n",
        "\n",
        "# =====================================\n",
        "# 2. Imports\n",
        "# =====================================\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import ta\n",
        "import os\n",
        "\n",
        "# =====================================\n",
        "# 3. Parameters\n",
        "# =====================================\n",
        "tickers = ['SPY','QQQ','IWM','EEM','TLT','GLD','XLK','XLF','XLE']  # Multi-ETF training\n",
        "prediction_symbol = 'E1VFVN30'  # VN30 ETF\n",
        "start_date = '2015-01-01'\n",
        "end_date = '2025-08-27'\n",
        "window_size = 60\n",
        "batch_size = 64\n",
        "epochs = 15\n",
        "lr = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# =====================================\n",
        "# 4. Download training data\n",
        "# =====================================\n",
        "data = yf.download(tickers, start=start_date, end=end_date, group_by='ticker', progress=False)\n",
        "print(\"Data downloaded for ETFs:\", len(data))\n",
        "\n",
        "# Flatten multi-index into one DataFrame\n",
        "frames = []\n",
        "for ticker in tickers:\n",
        "    df = data[ticker].copy()\n",
        "    df.columns = [col.lower() for col in df.columns]\n",
        "    df['ticker'] = ticker\n",
        "    frames.append(df.reset_index())\n",
        "df_all = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "# =====================================\n",
        "# 5. Feature Engineering\n",
        "# =====================================\n",
        "def add_features(df):\n",
        "    df['return'] = df['close'].pct_change()\n",
        "    df['volatility'] = df['return'].rolling(10).std()\n",
        "    df['ma_10'] = df['close'].rolling(10).mean()\n",
        "    df['ma_20'] = df['close'].rolling(20).mean()\n",
        "    # RSI\n",
        "    df['rsi_14'] = ta.momentum.RSIIndicator(df['close'], window=14).rsi()\n",
        "    # MACD\n",
        "    macd = ta.trend.MACD(df['close'])\n",
        "    df['macd'] = macd.macd()\n",
        "    df['macd_signal'] = macd.macd_signal()\n",
        "    df['macd_hist'] = macd.macd_diff()\n",
        "    return df\n",
        "\n",
        "df_all = df_all.groupby('ticker', group_keys=False).apply(add_features)\n",
        "df_all.dropna(inplace=True)\n",
        "\n",
        "feature_cols = ['close','volume','return','volatility','ma_10','ma_20','rsi_14','macd','macd_signal','macd_hist']\n",
        "target_col = 'close'\n",
        "\n",
        "# =====================================\n",
        "# 6. Prepare sequences for LSTM\n",
        "# =====================================\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = []\n",
        "for ticker in tickers:\n",
        "    df_t = df_all[df_all['ticker']==ticker]\n",
        "    X = df_t[feature_cols].values\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    for i in range(len(X_scaled)-window_size):\n",
        "        scaled_data.append((X_scaled[i:i+window_size], X_scaled[i+window_size,0])) # predict close price\n",
        "scaled_data = np.array(scaled_data, dtype=object)\n",
        "print(\"Sequences prepared:\", len(scaled_data))\n",
        "\n",
        "# Train/val/test split\n",
        "train_size = int(0.8 * len(scaled_data))\n",
        "val_size = int(0.1 * len(scaled_data))\n",
        "test_size = len(scaled_data) - train_size - val_size\n",
        "\n",
        "train_data = scaled_data[:train_size]\n",
        "val_data = scaled_data[train_size:train_size+val_size]\n",
        "test_data = scaled_data[train_size+val_size:]\n",
        "\n",
        "class StockDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.data[idx]\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "train_loader = DataLoader(StockDataset(train_data), batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(StockDataset(val_data), batch_size=batch_size)\n",
        "test_loader = DataLoader(StockDataset(test_data), batch_size=batch_size)\n",
        "\n",
        "# =====================================\n",
        "# 7. Define LSTM Model\n",
        "# =====================================\n",
        "class LSTMRegressor(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = out[:, -1, :]\n",
        "        return self.head(out)\n",
        "\n",
        "input_size = len(feature_cols)\n",
        "model = LSTMRegressor(input_size).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# =====================================\n",
        "# 8. Training loop\n",
        "# =====================================\n",
        "best_val_loss = float('inf')\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x_batch)\n",
        "        loss = criterion(output.squeeze(), y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()*x_batch.size(0)\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in val_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            output = model(x_batch)\n",
        "            loss = criterion(output.squeeze(), y_batch)\n",
        "            val_loss += loss.item()*x_batch.size(0)\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] Train Loss: {train_loss:.6f} Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'lstm_stock_model.pt')\n",
        "\n",
        "# =====================================\n",
        "# 9. Predict for E1VFVN30 next 7 days\n",
        "# =====================================\n",
        "df_pred = yf.download(prediction_symbol+'.VN', start='2024-01-01', end=end_date, progress=False)\n",
        "df_pred.columns = [col.lower() for col in df_pred.columns]\n",
        "df_pred = add_features(df_pred)\n",
        "df_pred.dropna(inplace=True)\n",
        "\n",
        "# Use last window for prediction\n",
        "X_full = df_pred[feature_cols].values\n",
        "X_scaled = scaler.fit_transform(X_full)\n",
        "\n",
        "last_seq = torch.tensor(X_scaled[-window_size:], dtype=torch.float32).unsqueeze(0).to(device)\n",
        "model.load_state_dict(torch.load('lstm_stock_model.pt'))\n",
        "model.eval()\n",
        "\n",
        "future_preds = []\n",
        "for _ in range(7):\n",
        "    with torch.no_grad():\n",
        "        pred = model(last_seq).item()\n",
        "    future_preds.append(pred)\n",
        "    # append predicted close and zeros for other features (simple iterative)\n",
        "    next_row = np.zeros((len(feature_cols),))\n",
        "    next_row[0] = pred\n",
        "    new_seq = torch.cat([last_seq[:,1:,:], torch.tensor(next_row, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)], dim=1)\n",
        "    last_seq = new_seq\n",
        "\n",
        "# Inverse scale predictions\n",
        "dummy = np.zeros((len(future_preds), len(feature_cols)))\n",
        "dummy[:,0] = future_preds\n",
        "future_prices = scaler.inverse_transform(dummy)[:,0]\n",
        "\n",
        "print(\"Predicted next 7 closes:\", future_prices)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(df_pred.index[-100:], df_pred['close'].values[-100:], label=\"History\")\n",
        "plt.plot(pd.date_range(df_pred.index[-1], periods=8, freq='B')[1:], future_prices, label=\"Forecast\", color='red')\n",
        "plt.title(f\"{prediction_symbol} - Last 100 days and next 7 predicted\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iZuyI0-2Zyns",
        "outputId": "974c4061-99e0-4162-f8cd-b284039be9e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        }
      },
      "id": "iZuyI0-2Zyns",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4225011921.py:37: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  data = yf.download(tickers, start=start_date, end=end_date, group_by='ticker', progress=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data downloaded for ETFs: 2678\n",
            "Sequences prepared: 23265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4225011921.py:66: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df_all = df_all.groupby('ticker', group_keys=False).apply(add_features)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15] Train Loss: 0.007761 Val Loss: 0.000413\n",
            "Epoch [2/15] Train Loss: 0.000590 Val Loss: 0.000273\n",
            "Epoch [3/15] Train Loss: 0.000485 Val Loss: 0.000235\n",
            "Epoch [4/15] Train Loss: 0.000423 Val Loss: 0.000211\n",
            "Epoch [5/15] Train Loss: 0.000358 Val Loss: 0.000165\n",
            "Epoch [6/15] Train Loss: 0.000337 Val Loss: 0.000196\n",
            "Epoch [7/15] Train Loss: 0.000328 Val Loss: 0.000251\n",
            "Epoch [8/15] Train Loss: 0.000317 Val Loss: 0.000282\n",
            "Epoch [9/15] Train Loss: 0.000307 Val Loss: 0.000376\n",
            "Epoch [10/15] Train Loss: 0.000270 Val Loss: 0.000138\n",
            "Epoch [11/15] Train Loss: 0.000263 Val Loss: 0.000133\n",
            "Epoch [12/15] Train Loss: 0.000269 Val Loss: 0.000194\n",
            "Epoch [13/15] Train Loss: 0.000262 Val Loss: 0.000130\n",
            "Epoch [14/15] Train Loss: 0.000238 Val Loss: 0.000125\n",
            "Epoch [15/15] Train Loss: 0.000246 Val Loss: 0.000145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4225011921.py:166: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df_pred = yf.download(prediction_symbol+'.VN', start='2024-01-01', end=end_date, progress=False)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'tuple' object has no attribute 'lower'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4225011921.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;31m# =====================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0mdf_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_symbol\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.VN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'2024-01-01'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m \u001b[0mdf_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0mdf_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0mdf_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o5nVtbJ5ZzBk"
      },
      "id": "o5nVtbJ5ZzBk",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}