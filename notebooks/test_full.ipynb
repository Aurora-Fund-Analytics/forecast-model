{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aurora-Fund-Analytics/forecast-model/blob/main/notebooks/test_full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd91904d",
      "metadata": {
        "id": "cd91904d"
      },
      "source": [
        "# Prediction model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "875ee909",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "875ee909"
      },
      "outputs": [],
      "source": [
        "!pip install torch yfinance pandas numpy scikit-learn ta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import json\n",
        "import time\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "try:\n",
        "    import ta\n",
        "except ImportError:\n",
        "    ta = None\n",
        "\n",
        "# Fix random seed\n",
        "def set_seed(seed: int = 42):\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "44XEZrCtKnCU",
        "outputId": "9de3cd34-cb0b-4e70-d89a-9ff85481cc01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "id": "44XEZrCtKnCU",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_ohlcv(tickers: List[str], start: str, end: str) -> pd.DataFrame:\n",
        "    df = yf.download(\n",
        "        tickers=tickers,\n",
        "        start=start,\n",
        "        end=end,\n",
        "        auto_adjust=True,\n",
        "        group_by='ticker',\n",
        "        threads=True,\n",
        "        progress=False,\n",
        "        interval='1d'\n",
        "    )\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        df = df.sort_index(axis=1)\n",
        "    else:\n",
        "        df = pd.concat({tickers[0]: df}, axis=1)\n",
        "    return df\n",
        "\n",
        "def add_features_for_ticker(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    d = df.copy()\n",
        "    d['ret_1'] = d['Close'].pct_change()\n",
        "    d['log_ret_1'] = np.log1p(d['ret_1'])\n",
        "\n",
        "    for w in (5, 10, 20, 60):\n",
        "        d[f'roll_mean_{w}'] = d['Close'].pct_change().rolling(w).mean()\n",
        "        d[f'roll_std_{w}'] = d['Close'].pct_change().rolling(w).std()\n",
        "        d[f'roll_min_{w}'] = d['Close'].rolling(w).min() / d['Close'] - 1.0\n",
        "        d[f'roll_max_{w}'] = d['Close'].rolling(w).max() / d['Close'] - 1.0\n",
        "        d[f'price_sma_{w}'] = d['Close'] / d['Close'].rolling(w).mean() - 1.0\n",
        "        d[f'vol_sma_{w}'] = d['Volume'] / d['Volume'].rolling(w).mean() - 1.0\n",
        "\n",
        "    if ta is not None:\n",
        "        try:\n",
        "            d['rsi_14'] = ta.momentum.RSIIndicator(close=d['Close'], window=14).rsi()\n",
        "            macd = ta.trend.MACD(close=d['Close'])\n",
        "            d['macd'] = macd.macd()\n",
        "            d['macd_signal'] = macd.macd_signal()\n",
        "            d['macd_hist'] = macd.macd_diff()\n",
        "        except Exception:\n",
        "            d[['rsi_14','macd','macd_signal','macd_hist']] = np.nan\n",
        "    else:\n",
        "        d[['rsi_14','macd','macd_signal','macd_hist']] = np.nan\n",
        "\n",
        "    d['vol_ret_1'] = d['Volume'].pct_change().replace([np.inf, -np.inf], np.nan)\n",
        "    d['hl_spread'] = (d['High'] - d['Low']) / d['Close']\n",
        "    d['y_target'] = d['log_ret_1'].shift(-1)\n",
        "\n",
        "    return d.dropna()\n",
        "\n",
        "def build_feature_matrix(df_multi, tickers):\n",
        "    rows = []\n",
        "    for t in tickers:\n",
        "        one = df_multi[t][['Open','High','Low','Close','Volume']].dropna()\n",
        "        f = add_features_for_ticker(one)\n",
        "        f['ticker'] = t\n",
        "        rows.append(f)\n",
        "    data = pd.concat(rows)\n",
        "    data['date'] = data.index\n",
        "    data = data.sort_values(['ticker','date']).reset_index(drop=True)\n",
        "    feature_cols = [c for c in data.columns if c not in ('y_target','ticker','date')]\n",
        "    return data, feature_cols\n"
      ],
      "metadata": {
        "id": "ZfDtr1xpLpLz"
      },
      "id": "ZfDtr1xpLpLz",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_sequences(data, feature_cols, window, horizon=1):\n",
        "    X_list, y_list = [], []\n",
        "    for t, grp in data.groupby('ticker', sort=False):\n",
        "        vals = grp[feature_cols + ['y_target']].values\n",
        "        for i in range(window, len(vals) - horizon + 1):\n",
        "            X_list.append(vals[i-window:i, :-1])\n",
        "            y_list.append(vals[i + horizon - 1, -1])\n",
        "    return np.array(X_list, dtype=np.float32), np.array(y_list, dtype=np.float32)\n",
        "\n",
        "class SeqDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        self.y = torch.from_numpy(y).float().view(-1, 1)\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "Kt07bdwALr6x"
      },
      "id": "Kt07bdwALr6x",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMRegressor(nn.Module):\n",
        "    def __init__(self, n_features, hidden=96, num_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(n_features, hidden, num_layers=num_layers,\n",
        "                            batch_first=True, dropout=dropout if num_layers > 1 else 0.0)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(hidden, hidden//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden//2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        last = out[:, -1, :]\n",
        "        return self.head(last)\n"
      ],
      "metadata": {
        "id": "pVXsfimjLvIC"
      },
      "id": "pVXsfimjLvIC",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total = 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(xb)\n",
        "        loss = criterion(pred, yb)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        total += loss.item() * xb.size(0)\n",
        "    return total / len(loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    preds, trues = [], []\n",
        "    total = 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        pred = model(xb)\n",
        "        loss = criterion(pred, yb)\n",
        "        total += loss.item() * xb.size(0)\n",
        "        preds.append(pred.cpu().numpy())\n",
        "        trues.append(yb.cpu().numpy())\n",
        "    preds = np.concatenate(preds).flatten()\n",
        "    trues = np.concatenate(trues).flatten()\n",
        "    mse = np.mean((preds - trues)**2)\n",
        "    mae = np.mean(np.abs(preds - trues))\n",
        "    dir_acc = np.mean(np.sign(preds) == np.sign(trues))\n",
        "    return total / len(loader.dataset), mse, mae, dir_acc\n"
      ],
      "metadata": {
        "id": "TdrPaP8ALwl6"
      },
      "id": "TdrPaP8ALwl6",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Config\n",
        "tickers = [\"SPY\",\"QQQ\",\"IWM\",\"EEM\",\"TLT\",\"GLD\"]\n",
        "start = \"2005-01-01\"\n",
        "end = \"2025-08-25\"\n",
        "window = 60\n",
        "horizon = 1\n",
        "batch_size = 128\n",
        "hidden = 96\n",
        "layers = 2\n",
        "dropout = 0.2\n",
        "epochs = 35\n",
        "lr = 1e-3\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Download & prepare data\n",
        "ohlcv = download_ohlcv(tickers, start, end)\n",
        "data, feature_cols = build_feature_matrix(ohlcv, tickers)\n",
        "\n",
        "# Train/Val/Test split\n",
        "data_sorted = data.sort_values('date')\n",
        "n = len(data_sorted)\n",
        "n_test = int(0.1*n)\n",
        "n_val = int(0.1*n)\n",
        "train_df = data_sorted.iloc[:n-n_val-n_test]\n",
        "val_df = data_sorted.iloc[n-n_val-n_test:n-n_test]\n",
        "test_df = data_sorted.iloc[n-n_test:]\n",
        "\n",
        "# Scale\n",
        "scaler = StandardScaler().fit(train_df[feature_cols])\n",
        "train_df[feature_cols] = scaler.transform(train_df[feature_cols])\n",
        "val_df[feature_cols] = scaler.transform(val_df[feature_cols])\n",
        "test_df[feature_cols] = scaler.transform(test_df[feature_cols])\n",
        "\n",
        "# Build sequences\n",
        "X_train, y_train = make_sequences(train_df, feature_cols, window)\n",
        "X_val, y_val = make_sequences(val_df, feature_cols, window)\n",
        "X_test, y_test = make_sequences(test_df, feature_cols, window)\n",
        "\n",
        "train_loader = DataLoader(SeqDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(SeqDataset(X_val, y_val), batch_size=batch_size)\n",
        "test_loader = DataLoader(SeqDataset(X_test, y_test), batch_size=batch_size)\n",
        "\n",
        "# Model\n",
        "model = LSTMRegressor(len(feature_cols), hidden=hidden, num_layers=layers, dropout=dropout).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "best_val = float('inf')\n",
        "wait = 0\n",
        "patience = 7\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    tr_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "    val_loss, val_mse, val_mae, val_da = evaluate(model, val_loader, criterion)\n",
        "    print(f\"Epoch {epoch:03d} | Train {tr_loss:.6f} | Val {val_loss:.6f} | MSE {val_mse:.6f} | MAE {val_mae:.6f} | DirAcc {val_da:.3f}\")\n",
        "    if val_loss < best_val:\n",
        "        best_val = val_loss\n",
        "        best_state = model.state_dict()\n",
        "        wait = 0\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= patience:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n",
        "\n",
        "model.load_state_dict(best_state)\n",
        "\n",
        "# Test\n",
        "test_loss, test_mse, test_mae, test_da = evaluate(model, test_loader, criterion)\n",
        "print(f\"Test -> MSE: {test_mse:.6f}, MAE: {test_mae:.6f}, DirAcc: {test_da:.3f}\")\n"
      ],
      "metadata": {
        "id": "0jiwDdn9L056",
        "outputId": "1f2ff78e-a127-4471-cdd6-23611279991f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "0jiwDdn9L056",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3970487571.py:31: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_df[feature_cols] = scaler.transform(train_df[feature_cols])\n",
            "/tmp/ipython-input-3970487571.py:32: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  val_df[feature_cols] = scaler.transform(val_df[feature_cols])\n",
            "/tmp/ipython-input-3970487571.py:33: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_df[feature_cols] = scaler.transform(test_df[feature_cols])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | Train 0.000647 | Val 0.000191 | MSE 0.000191 | MAE 0.010494 | DirAcc 0.508\n",
            "Epoch 002 | Train 0.000310 | Val 0.000188 | MSE 0.000188 | MAE 0.010457 | DirAcc 0.498\n",
            "Epoch 003 | Train 0.000260 | Val 0.000186 | MSE 0.000186 | MAE 0.010463 | DirAcc 0.500\n",
            "Epoch 004 | Train 0.000230 | Val 0.000186 | MSE 0.000186 | MAE 0.010431 | DirAcc 0.490\n",
            "Epoch 005 | Train 0.000209 | Val 0.000183 | MSE 0.000183 | MAE 0.010351 | DirAcc 0.510\n",
            "Epoch 006 | Train 0.000201 | Val 0.000191 | MSE 0.000191 | MAE 0.010567 | DirAcc 0.499\n",
            "Epoch 007 | Train 0.000194 | Val 0.000191 | MSE 0.000191 | MAE 0.010497 | DirAcc 0.495\n",
            "Epoch 008 | Train 0.000192 | Val 0.000187 | MSE 0.000187 | MAE 0.010411 | DirAcc 0.494\n",
            "Epoch 009 | Train 0.000191 | Val 0.000183 | MSE 0.000183 | MAE 0.010321 | DirAcc 0.500\n",
            "Epoch 010 | Train 0.000191 | Val 0.000183 | MSE 0.000183 | MAE 0.010323 | DirAcc 0.512\n",
            "Epoch 011 | Train 0.000189 | Val 0.000193 | MSE 0.000193 | MAE 0.010515 | DirAcc 0.495\n",
            "Epoch 012 | Train 0.000189 | Val 0.000182 | MSE 0.000182 | MAE 0.010293 | DirAcc 0.523\n",
            "Epoch 013 | Train 0.000189 | Val 0.000185 | MSE 0.000185 | MAE 0.010368 | DirAcc 0.510\n",
            "Epoch 014 | Train 0.000188 | Val 0.000183 | MSE 0.000183 | MAE 0.010322 | DirAcc 0.508\n",
            "Epoch 015 | Train 0.000188 | Val 0.000184 | MSE 0.000184 | MAE 0.010350 | DirAcc 0.494\n",
            "Epoch 016 | Train 0.000187 | Val 0.000186 | MSE 0.000186 | MAE 0.010376 | DirAcc 0.510\n",
            "Epoch 017 | Train 0.000186 | Val 0.000186 | MSE 0.000186 | MAE 0.010384 | DirAcc 0.495\n",
            "Epoch 018 | Train 0.000186 | Val 0.000189 | MSE 0.000189 | MAE 0.010446 | DirAcc 0.498\n",
            "Epoch 019 | Train 0.000183 | Val 0.000184 | MSE 0.000184 | MAE 0.010341 | DirAcc 0.497\n",
            "Early stopping.\n",
            "Test -> MSE: 0.000132, MAE: 0.008169, DirAcc: 0.554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"artifacts\", exist_ok=True)\n",
        "torch.save({\n",
        "    'state_dict': model.state_dict(),\n",
        "    'feature_cols': feature_cols,\n",
        "    'window': window,\n",
        "    'scaler_mean_': scaler.mean_.tolist(),\n",
        "    'scaler_scale_': scaler.scale_.tolist()\n",
        "}, \"artifacts/lstm_stock_model.pt\")\n",
        "\n",
        "# TorchScript version\n",
        "example = torch.from_numpy(X_test[:1]).float().to(device)\n",
        "traced = torch.jit.trace(model, example)\n",
        "traced.save(\"artifacts/lstm_stock_model_traced.pt\")\n",
        "print(\"Model saved in artifacts/\")"
      ],
      "metadata": {
        "id": "p7bDXiExL3Yy",
        "outputId": "c943c1e6-0b36-4806-f288-6ed86c714cad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "p7bDXiExL3Yy",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved in artifacts/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load artifacts\n",
        "artifacts = torch.load(\"artifacts/lstm_stock_model.pt\", map_location=device)\n",
        "feature_cols = artifacts['feature_cols']\n",
        "window = artifacts['window']\n",
        "scaler_mean = np.array(artifacts['scaler_mean_'])\n",
        "scaler_scale = np.array(artifacts['scaler_scale_'])\n",
        "\n",
        "# Load model\n",
        "model = LSTMRegressor(n_features=len(feature_cols), hidden=96, num_layers=2, dropout=0.2)\n",
        "model.load_state_dict(artifacts['state_dict'])\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Step A: Download new data for E1VFVN30\n",
        "ticker = \"E1VFVN30.VN\"\n",
        "start = \"2024-01-01\"\n",
        "end = \"2025-08-27\"\n",
        "df_new = yf.download(ticker, start=start, end=end, auto_adjust=True, progress=False)\n",
        "df_new = df_new[['Open','High','Low','Close','Volume']].dropna()"
      ],
      "metadata": {
        "id": "tXhyiIAiOw2F"
      },
      "id": "tXhyiIAiOw2F",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Step B: Feature engineering\n",
        "def add_features_for_ticker(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    d = df.copy()\n",
        "    d['ret_1'] = d['Close'].pct_change()\n",
        "    d['log_ret_1'] = np.log1p(d['ret_1'])\n",
        "\n",
        "    for w in (5, 10, 20, 60):\n",
        "        d[f'roll_mean_{w}'] = d['Close'].pct_change().rolling(w).mean()\n",
        "        d[f'roll_std_{w}'] = d['Close'].pct_change().rolling(w).std()\n",
        "        d[f'roll_min_{w}'] = d['Close'].rolling(w).min() / d['Close'] - 1.0\n",
        "        d[f'roll_max_{w}'] = d['Close'].rolling(w).max() / d['Close'] - 1.0\n",
        "        d[f'price_sma_{w}'] = d['Close'] / d['Close'].rolling(w).mean() - 1.0\n",
        "        d[f'vol_sma_{w}'] = d['Volume'] / d['Volume'].rolling(w).mean() - 1.0\n",
        "\n",
        "    if ta is not None:\n",
        "        try:\n",
        "            d['rsi_14'] = ta.momentum.RSIIndicator(close=d['Close'], window=14).rsi()\n",
        "            macd = ta.trend.MACD(close=d['Close'])\n",
        "            d['macd'] = macd.macd()\n",
        "            d['macd_signal'] = macd.macd_signal()\n",
        "            d['macd_hist'] = macd.macd_diff()\n",
        "        except Exception:\n",
        "            d[['rsi_14','macd','macd_signal','macd_hist']] = np.nan\n",
        "    else:\n",
        "        d[['rsi_14','macd','macd_signal','macd_hist']] = np.nan\n",
        "\n",
        "    d['vol_ret_1'] = d['Volume'].pct_change().replace([np.inf, -np.inf], np.nan)\n",
        "    d['hl_spread'] = (d['High'] - d['Low']) / d['Close']\n",
        "    d['y_target'] = d['log_ret_1'].shift(-1)\n",
        "\n",
        "    return d.dropna()\n",
        "\n",
        "df_feat = add_features_for_ticker(df_new)\n",
        "\n",
        "# Step C: Apply scaler\n",
        "X_full = df_feat[feature_cols].values\n",
        "X_scaled = (X_full - scaler_mean) / scaler_scale\n",
        "\n",
        "# Step D: Get last 'window' sequence\n",
        "if len(X_scaled) < window:\n",
        "    raise ValueError(\"Not enough data for window size\")\n",
        "last_seq = X_scaled[-window:].reshape(1, window, -1)\n",
        "\n",
        "# Step E: Predict next 5 days (simulate)\n",
        "predicted_prices = []\n",
        "current_close = df_feat['Close'].iloc[-1]\n",
        "\n",
        "seq = last_seq.copy()\n",
        "for _ in range(5):  # next 5 days\n",
        "    x_tensor = torch.from_numpy(seq).float().to(device)\n",
        "    with torch.no_grad():\n",
        "        pred_log_ret = model(x_tensor).item()  # predicted log return\n",
        "    next_close = current_close * np.exp(pred_log_ret)  # convert log return to price\n",
        "    predicted_prices.append(next_close)\n",
        "\n",
        "    # Update seq for next prediction: shift left, append predicted feature row\n",
        "    # For simplicity, append same features except 'Close' updated\n",
        "    new_row = seq[0, -1, :].copy()\n",
        "    # Replace normalized close-related features:\n",
        "    new_row[feature_cols.index('log_ret_1')] = pred_log_ret  # naive approximation\n",
        "    seq = np.roll(seq, -1, axis=1)\n",
        "    seq[0, -1, :] = new_row\n",
        "\n",
        "    current_close = next_close\n",
        "\n",
        "predicted_prices"
      ],
      "metadata": {
        "id": "JCXdjiLIMPnM",
        "outputId": "be3beda0-0746-4948-d422-3020f01ee793",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "id": "JCXdjiLIMPnM",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Not enough data for window size",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7533736.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# Step D: Get last 'window' sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Not enough data for window size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0mlast_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_scaled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Not enough data for window size"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vO9Daxk-NKyE"
      },
      "id": "vO9Daxk-NKyE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}